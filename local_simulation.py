import sys
import socket
import os
import time
import threading
import random
import multiprocessing
import json
import subprocess
import struct
import numpy as np
import re
import matplotlib.pyplot as plt
import time
import math
import pickle
import networkx as nx
from networkx.algorithms.clique import find_cliques as maximal_cliques
from itertools import combinations
from ortools.linear_solver import pywraplp 

from tqdm import tqdm
from mininet.node import Controller
from mininet.log import setLogLevel, info
from mn_wifi.net import Mininet_wifi
from mn_wifi.cli import CLI
from mn_wifi.link import wmediumd
from mn_wifi.wmediumdConnector import interference
from datetime import datetime, timedelta
from collections import defaultdict


"""Convert throughput from KiB, MiB, B to KB."""
def convert_to_KB(throughput):
    match = re.match(r"([0-9.]+)([KMG]i?B?/s?)", throughput.strip())
    if match:
        value = float(match.group(1))
        unit = match.group(2).lower()
        if 'ki' in unit:  # KiB
            return value
        elif 'mi' in unit:  # MiB
            return value * 1024 
        elif 'b' in unit:  # B
            return value / 1024
        else:
            return 0
    return 0

"""Extract the latest throughput from file."""
def get_latest_throughput(filename):
    with open(filename, 'r') as f:
        lines = f.readlines()
    
    # Get the last non-empty line with throughput
    latest_line = lines[-1].strip()
    
    # Extract the throughput value
    throughput_value = latest_line.strip('()')  # Remove the square brackets
    
    # Convert throughput to bytes
    throughput_KB = convert_to_KB(throughput_value)
    
    return throughput_KB 

class sensor_cluster():
    def _read_temperature_data(self, file_path, sensor_id):
        with open(file_path, 'r') as file:
            # Skip the first 4 lines
            data = []
            bytes_received = 0
            for line in file:
                bytes_received += len(line)
                # Ignore filler lines
                if line[0] == 'G':
                    continue
                try:
                    # Split the line and try to convert the temperature value (9th column, index 8) to float
                    temperature = np.float32(line.strip().split(',')[8])
                    data.append(temperature)
                except (ValueError, IndexError):
                    # If conversion fails or the line doesn't have enough columns, skip this line
                    continue
            self._throughputs[sensor_id] = bytes_received / 1024 / self._observation_time

        return np.array(data)

    """
    Split dataset into chunks. A chunk corresponds to a transmissions worth of data.
    Note: The chunks returned must be padded before transmission to the correct size.

    Args:
        dataset_dir (string): Path to dataset file
        sensor_id (int): ID of the sensor associcated to the dataset file
        file_lines_per_transmission (int): How many file lines to include per transmission 

    Returns:
        String List: List of chunks 
    """

    def _preprocess_dataset_into_chunks(self, dataset_dir, sensor_id, file_lines_per_chunk, data_offset):
        # Cache the dataset into memory 
        with open(dataset_dir, 'r') as file:
            lines = file.readlines()

        # Get the number of lines in the file 
        file_size = len(lines)

        # Get the number of chunks the data set can split into
        num_chunks_in_file = file_size // file_lines_per_chunk

        if num_chunks_in_file < self._chunks_to_send:
            print('The number of chunks to send or the number of file lines per chunk must be decreased; not enough data')

        # Split file into chunks
        chunks = [lines[i:i + file_lines_per_chunk] for i in range(0, len(lines), file_lines_per_chunk)]
        chunks = [''.join(chunk) for chunk in chunks[data_offset*self._chunks_to_send: (data_offset + 1) * self._chunks_to_send]]

        # Pad chunks to correct size
        #filler = 'G' * (self._transmission_size - 1) + '\n'
        #chunks = [(chunk + filler[len(chunks):]).encode('utf-8')  for chunk in chunks]

        return chunks
   
    """
    Get an observation for the scheduler.

    Returns:
        tuple: (rates_id, similarity, energy, throughputs, reward)
            rates_id - ID of rates used during the observation
            similarity - num_sensor x num_sensor matrix where entry i,j denotes wheter the data generated by sensor i was similar to the data generated by sensor j
            energy - percent of energy remaining for each sensor
            throughputs - vector of average throughput for each sensor
            reward - reward according to reward function
    """

    def get_obs(self):
        print('\n\nGetting observation')

        # Dict with keys as awake sensor ids and values as the data received by the cluster head from a sensor
        temperature_data = {}

        for i in range(self._num_sensors):
            # Name of file where transmisions recieived by the cluster head from sensor i is stored
            file_name = f'ch_received_from_sensor_{self._sensor_ids[i]}.txt'

            # Create a copy of the original file
            subprocess.run(["cp", f'{self._log_directory}/{file_name}', f'{self._log_directory}/.{file_name}'])
            file_path = os.path.join(self._log_directory, f'.{file_name}')

            # Clear the file for future transmissions
            # TODO: Lock before clearing?
            with open(f'{self._log_directory}/{file_name}', 'r+') as file:
                file.truncate(0)
            
            # Read the temperature data for sensor i from the copied file
            sensor_data = self._read_temperature_data(file_path, i)
            if len(sensor_data) > 0:
                temperature_data[i] = sensor_data
                data_arrived = True

        similarity = np.zeros((self._num_sensors, self._num_sensors))
        reward = 0

        awake_sensors = list(temperature_data.keys())
         
        # Create a graph with veritices representing sensors and edges denoting similarity
        G = nx.Graph()

        print(f'Awake sensors: {awake_sensors}')
        
        for i in range(self._num_sensors):
            G.add_node(i)

        for i in range(len(awake_sensors)):
            print(f"throughput in bytes for sensor {awake_sensors[i]}: {self._throughputs[i]}")
            # Get the average throughput in KiB/s 
            #self._throughputs[awake_sensors[i]] = len(temperature_data[awake_sensors[i]]) / 1024 / self._observation_time

            for j in range(i + 1, len(awake_sensors)):
                # TODO: Currently only using first data point for similiarity; need to add timestamps to data for more robust similarity checking
                similarity[awake_sensors[i], awake_sensors[j]] = int(temperature_data[awake_sensors[i]][0] - temperature_data[awake_sensors[j]][0] <= self._similarity_threshold)
                if similarity[awake_sensors[i],awake_sensors[j]] == 1:
                    print(f'Node {awake_sensors[i]} ~ Node {awake_sensors[j]}')
                    G.add_edge(awake_sensors[i],awake_sensors[j])
            
        print(f'Rates (Kib/s) : {[a for a in self.transmission_freq_idxs]}')
        max_throughput = (max(self._throughputs))

        self._max_throughput = max(max(self._throughputs), self._max_throughput)
        self._max_total_throughput = max(self._max_total_throughput, sum(self._throughputs))

        if self._max_throughput == 0:
            return (similarity, [e / self._full_energy for e in self._energy], self._throughputs, 0)

        
        print(f'Throughputs (Kib/s) : {self._throughputs}')
        print(f'Total throughput over observation: {np.sum(self._throughputs)} KiB/s')
        #print(f'Maximum attainable throughput for a sensor (Kib/s): {max_throughput}')
        #print(f'Maximum attainable throughput all sensors (Kib/s): {self._max_throughput}')

        # Get a maximal clique cover for G
        maximal_clique_cover = list(maximal_cliques(G)) 
        print(f'maximal_clique_cover: {maximal_clique_cover}')
        print(f'Max throughput in clique: {[max([self._throughputs[node] for node in clique]) for clique in maximal_clique_cover]}')

        # Reward high throughput of non-redudant data
        #max_clique_throughputs = [max([self._throughputs[node] / (max_throughput) for node in clique]) for clique in maximal_clique_cover] 
        #reward = 0.5 * (np.median(max_clique_throughputs) - 1)
        #reward += 0.5 * ((np.sum(self._throughputs) / (self._num_sensors * max_throughput)) - 1)
        
        max_clique_throughputs = [max([self._throughputs[node] / (self._max_throughput) for node in clique]) for clique in maximal_clique_cover] 
        clique_throughput_reward = 0.5 * (np.median(max_clique_throughputs) - 1)
        throughput_reward = 0.5 * ((np.sum(self._throughputs) / (self._max_total_throughput)) - 1)
        reward = clique_throughput_reward + throughput_reward

        if np.sum(self._throughputs) > self._max_throughput:
            self._max_throughput = np.sum(self._throughputs) 

        print(f'Weighted avg energy: {np.average(self._energy)/100}')
        print(f'Reward: {reward}')
        print(f'Clique reward: {clique_throughput_reward}')
        print(f'Throughput reward: {throughput_reward}')

        #for i in range(self._num_sensors):
            #self.throughput_log[i].append(self._throughputs[i])
            #self.energy_log[i].append(self._energy[i])
            #self.rate_log[i].append(self.transmission_freq_idxs[i])

        #self.clique_log.append(maximal_clique_cover)

        #self.reward_log.append(reward)
        #self.clique_reward_log.append(clique_throughput_reward)
        #self.throughput_reward_log.append(throughput_reward)
        
        return (similarity, [e / self._full_energy for e in self._energy], self._throughputs, reward)
   
    def _generate_mac(self, identifier):
        base_mac = '00:00:00:00'
        return f'{base_mac}:{self._cluster_id:02x}:{identifier:02x}'
   
    """
    Create Mininet topology
    """
    def _create_topology(self):
        #build network
        #self._net = Mininet_wifi(controller=Controller, link=wmediumd, wmediumd_mode=interference)
        controller = Controller('c{self._cluster_id}', port=6653+self._cluster_id)
        self._net = Mininet_wifi(link=wmediumd, wmediumd_mode=interference)

        info("*** Creating nodes\n")
        print(f'ap mac: {self._generate_mac(1)}')
        #create accesspoint
        ap = self._net.addAccessPoint(f'ap{self._cluster_id}', ssid=f'c{self._cluster_id}-ssid', mode='g', channel='5', position='50,50,0', cpu=1, mem=1024*2, mac=self._generate_mac(1))
       
        #create cluster head
        print(f'cluster head mac: {self._generate_mac(2)}')
        self._cluster_head = self._net.addStation(f'ch{self._cluster_id}', ip=f'192.168.{self._cluster_id}.100/24',
                                      range='150', position='30,30,0', cpu=1, mem=1024*2, mac=self._generate_mac(2))

        #create 10 sensors
        self._sensors = []
        for i in range(self._num_sensors):
            ip_address = f'192.168.{self._cluster_id}.{i + 1}/24'
            print(f'sensor mac: {self._generate_mac(i+3)}')

            self._sensors.append(self._net.addStation(f'c{self._cluster_id}s{i}', ip=ip_address,
                                          range='116', position=f'{-30 - i},-30,0', cpu=1, mem=1024*2, mac=self._generate_mac(i+3)))

        
        
        info("*** Adding Controller\n")
        #self._net.addController(f'c{self._cluster_id}')
        self._net.addController(controller)

        info("*** Configuring wifi nodes\n")
        self._net.configureWifiNodes()

        #self._net.setPropagationModel(model='logDistance', exp=2)

    def __init__(self, cluster_id, sensor_ids, transmission_size=2*1024, transmission_frame=1, observation_time=10, file_lines_per_chunk=10, log_directory='data/log', dataset_directory='data/towerdataset', data_offset=0):
        # Connection and configuration parameters
        self._cluster_id = cluster_id

        print(f"Cluster id: {cluster_id}")
        self._transmission_size = transmission_size # in bytes
        self._sensor_ids = sensor_ids
        self._num_sensors = len(sensor_ids)
        self._similarity_threshold = 1
        self._throughputs = [0 for i in range(self._num_sensors)]
        self._chunks_to_send = 100000
        self._observation_time = observation_time # The number of seconds between each observation
        self._max_throughput = 0
        self._max_total_throughput = 0
        self._transmission_frame=transmission_frame

        # Directories
        self._log_directory = log_directory
        self._dataset_directory = dataset_directory
        self._output_dir = f'{self._log_directory}/output/extracted_data' # Where to output logs 
       
        # Rate configuration
        self._rates_id = 0
        self.transmission_freq_idxs = multiprocessing.Array('i', [2] * self._num_sensors)
        self._transmission_frequencies = np.array([0, 1, 2, 4]) # possible transmit frequencies transmissions per second)

        # Initalize semaphores and events 
        #self._update_rates = threading.Semaphore(num_sensors) 
        #self._transmit_data_status = [threading.Event() for _ in range(num_sensors)]
        #self._ready_to_transmit = [threading.Event() for _ in range(num_sensors)]

        #self._update_rates_status = [threading.Event() for _ in range(num_sensors)]
        #for update_rate_status in self._update_rates_status:
        #    update_rate_status.set()
        #self._bytes_received = [0 for _ in range(num_sensors)]
        #for transmit_status in self._transmit_data_status:
        #    transmit_status.clear()

        # Energy configuration
        self._full_energy = 100
        self._recharge_time = 10 * transmission_frame
        self._recharge_threshold = 20
        self._energy = [self._full_energy for _ in range(self._num_sensors)]

        # Logs for tracking performance 
        self.rate_log = [[] for _ in range(self._num_sensors)]
        self.energy_log = [[] for _ in range(self._num_sensors)]
        self.throughput_log = [[] for _ in range(self._num_sensors)]
        self.reward_log = []
        self.clique_reward_log = []
        self.throughput_reward_log = []

        self.clique_log = []

        # RL parameters
        self._alpha = 3
        self._beta = 0.5

        # Delete log folder if already it exists
        subprocess.run(["rm", "-rf", log_directory])
        os.makedirs(log_directory)
        os.makedirs(log_directory + '/tmp')

        if not os.path.exists(dataset_directory):
            print("Dataset directory does not exist. Exiting now")
            exit()

        # Initalize the dataset for each sensor 
        self._datasets = {}
        for i in range(self._num_sensors):
            tower_number = i + 2  # Start from tower2 
            file_path = f'{dataset_directory}/tower{tower_number}Data_processed.csv'
            if os.path.exists(file_path):
                self._datasets[i] = self._preprocess_dataset_into_chunks(file_path, i, file_lines_per_chunk, data_offset)
                self._datasets[i] = self._datasets[i] + self._datasets[i]
            else:
                print(f"Warning: Dataset file not found for sensor {i}: {file_path}")
                self._datasets[i] = []

        self._create_topology()


    """
    Send message from a sensor to the cluster head

    Args:
        sensor (station): sensor to send message from
        ch_ip (string): cluster head ip
        sensor_id (int): id of sensor
    """

    def _send_messages_to_cluster_head(self, sensor, ch_ip, sensor_id):
        # Get the chunks corresponding to this sensor
        chunks = self._datasets[sensor_id] # Every chunk contains enough data for transmission at the highest rate for an observation period 
        info(f"Number of Chunks: {len(chunks)}\n")

        if not chunks:
            info(f"Sensor {sensor_id}: No data available. Skipping send_messages.\n")
            return
        
        # Create a file to store the packets received by the cluster head
        sensor.cmd(f'touch {self._log_directory}/sensor_{sensor_id}_log.txt')
        sensor.cmd(f'touch {self._log_directory}/test.txt')

       
        # Track the number of chunks sent 
        chunks_sent = 0

        # Index of the next chunk to send 
        next_chunk_idx = 0

        # Port to communictae with cluser head on 
        port = 5001 + sensor_id + self._cluster_id*100
        
        # Initalize the sensors energy and the recharge count 
        energy = self._full_energy
        self._energy[sensor_id] = self._full_energy
        charge_count = 0

        # Log the initial transmission rate
        self.rate_log[sensor_id].append(self.transmission_freq_idxs[sensor_id])

        filler = 'G' * (self._transmission_size - 1) + '\n'

        while next_chunk_idx < len(chunks):
            # Recharge sensor if energy is below the recharge threshold
            if self._energy[sensor_id] < self._recharge_threshold:
                charge_count += 1
                recharge_time = time.time()
                rechar_time_stamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(recharge_time))
                #print(f'Cluster {self._cluster_id}, Sensor {sensor_id}: Energy {self._energy[sensor_id]} below threshold ({self._recharge_threshold}). Recharging...')
                time.sleep(self._recharge_time)
                next_chunk_idx += (self._recharge_time / self._transmission_frame) // max(self._transmission_frequencies)
                                
                # Update the sensors energy and log the new energy level
                self._energy[sensor_id] = self._full_energy
                sensor.energy = energy

                #print(f'Cluster {self._cluster_id}, Sensor {sensor_id}: Energy Recharged to full energy ({self._full_energy}), Current time: {recharge_time}, Charge Count: {charge_count}. Resuming operations.')
                
                if next_chunk_idx >= len(chunks):
                    self._transmit_data_status[sensor_id].set()
                    break

            # Check for rate update before transmitting the next chunck
            #self._update_rates_status[sensor_id].wait()
            #self._update_rates_status[sensor_id].clear()

            # Get the current transmission rate
            transmit_rate = self._transmission_frequencies[self.transmission_freq_idxs[sensor_id]] 

            # Store the current time
            transmission_start_time = time.time()
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(transmission_start_time))
            ms = int((transmission_start_time - time.time()) * 1000)

            if transmit_rate == 0:
                next_chunk_idx += int(max(self._transmission_frequencies))
                #self._energy[sensor_id] -= 0.3 * self._transmission_frame
                #print(f"Sensor {sensor_id}: Skipped sending chunk {chunks_sent} due to rate 0 at {timestamp}.{ms:03d}\n")

                time.sleep(self._transmission_frame)
                continue

            # Get the chunk to send
            info(f"next chunk idx: {next_chunk_idx}")
            chunk = chunks[int(next_chunk_idx)] 
         
            # Send chunk
            cmd = f'printf "{chunk}\n{filler[len(chunk):]}\n" | nc -v -w0 -u {ch_ip} {port} >> tmp/nc{sensor_id} 2>&1 &'
            sensor.cmd(cmd)

            #self._energy[sensor_id] -= 4 * self._transmission_frame 
            #print(f"Sensor {sensor_id}: Sent chunk {chunks_sent} of size {(len(chunk) / 1024):.2f} KiB at {timestamp}.{ms:03d}\n")

            chunks_sent += 1

            transmission_time = time.time() - transmission_start_time
            #print(f'Transmission time: {transmission_time}\n')

            sleep_time = self._transmission_frame / transmit_rate
            
            """
            Chunks correspond with data for units of time i.e. chunk[0] corresponds to time 0, chunk[1] corresponds to time 1, 
            and so on. (max(self._transmission_rates) * sleep_time) corresponds to the number of chunks corresponding to the 
            next sleep_time seconds. All but the first of these chunks are not transmitted. 
            """

            next_chunk_idx += (max(self._transmission_frequencies) / transmit_rate) - 1
            time.sleep(sleep_time)

        info(f"Sensor {sensor_id}: Finished sending messages\n")
        self._transmit_data_status[sensor_id].set()


    """
    Start background netcat listining process for cluster head

    Args:
        node (station): cluster head 
    """
    def _receive_messages(self, node):
        base_output_file = f'{self._log_directory}/ch_received_from_sensor'

        for i in range(self._num_sensors):
            # save the data received from a sensor to their respective file
            output_file = f'{base_output_file}_{self._sensor_ids[i]}.txt'
            node.cmd(f'touch {output_file}')

            #listen_cmd = f'nohup nc -n -v -ulk -p {5001 + i} 2> {self._log_directory}/listen_err{i} | pv -m 1 -F "%a" >> {output_file} 2>{self._log_directory}/sensor_{self._sensor_ids[i]}_throughput.txt &'
           # node.cmd(listen_cmd)

            node.cmd(f'nc -n -vv -ul -p {5001 + i} -k >> {output_file} 2> {self._log_directory}/listen_err &')

            #node.cmd(f'echo "{listen_cmd}" | bash 2>> {self._log_directory}/listen_cmd_err.txt')
            info(f"Receiver: Started listening on port {5001 + i + self._cluster_id*100} for sensor {i}\n")

        node.cmd("iw dev wlan0 set type monitor")
        node.cmd("ifconfig wlan0 up")
        #capture the network by pcap
        pcap_file = f'{self._log_directory}/capture.pcap'
        node.cmd(f'tcpdump -i {node.defaultIntf().name} -n udp portrange {5001 + self._cluster_id*100}-{5001+self._num_sensors-1 + self._cluster_id*100} -U -w {pcap_file} &')
        info(f"Receiver: Started tcpdump capture on ports 5001-{5001+self._num_sensors-1 + self._cluster_id*100}\n")

    """
    Kill netcat listining process for cluster head

    Args:
        node (station): cluster head 
    """
    def _stop_receivers(self, node):
        #stop receiver
        node.cmd('pkill -f "nc -ul"')
        node.cmd('pkill tcpdump')
        info("Stopped all nc receivers and tcpdump\n")

    """
    Begin message transmission from sensors to cluster head and reap all threads after transmission concludes.   
    """
    def start(self):
        print("STARTING")
        self._net.build()
        self._net.start()

        info("*** Setting up communication flow\n")
        try:
            info("*** Starting receivers\n")
            print("Starting receivers")

            
            # Activate cluster head listening threads
            receive_thread = threading.Thread(target=self._receive_messages, args=(self._cluster_head,))
            receive_thread.start()

            self._start_time = time.time()

            # Give listening threads time to start
            time.sleep(10) 
            
            print("Starting senders")
            info("*** Starting senders\n")
            sender_threads = []
            listener_threads = []
            ch_ip = f'192.168.{self._cluster_id}.100'

            for i, sensor in enumerate(self._sensors):
                tcpdump_file = f'{self._log_directory}/tcpdump_sender_sensor{i}.pcap'
                sensor.cmd(f'tcpdump -i s{i}-wlan0 -w {tcpdump_file} &')
                
                thread = threading.Thread(target=self._send_messages_to_cluster_head, args=(sensor, ch_ip, i))
                thread.start()
                sender_threads.append(thread)

            time.sleep(self._observation_time)
           
            # Give cluster head time to receive data
            time.sleep(2 * self._observation_time)

            for thread in sender_threads:
                thread.join()

            print(f'Simulation complete; saving data\n')
            with open('figure_data.pkl', 'wb') as file:
                pickle.dump((self._sensor_ids, self._rate_frequencies, self.rate_log, self.energy_log, self.throughput_log, self.reward_log, self.clique_reward_log, self.throughput_reward_log, self.clique_log), file)
            print(f'Pickle dump succesfuly made\n')

            self._stop_receivers(cluster_head)
            receive_thread.join()

            for thread in listener_threads:
                thread.join()
        
            for sensor in sensors:
                sensor.cmd('pkill tcpdump')
            cluster_head.cmd('pkill nc')

            self._plot_energy()
            self._plot_throughput()
            self._plot_rates(3)
            self._create_plot(self, self.rewards, 'rewards', self.timestaps, 'Time (seconds)', 'Reward', 'Reward over Time')
            self._create_plot(self, self.similarity, 'Similarity Penalty', self.timestaps, 'Time (seconds)', 'Penalty', 'Similarity Penalty over Time') 
            
        except Exception as e:
            info(f"*** Error occurred during communication: {str(e)}\n")
            
        info("*** Running CLI\n")
        CLI(self._net)

        info("*** Stopping network\n")
        self._net.stop()

        self._server.close()

    def _parse_tcpdump_output(self, file_path):
        print("Starting to parse tcpdump output...")
        start_time = time.time()
        udp_pattern = re.compile(r'(\d{2}:\d{2}:\d{2}\.\d+)\sIP\s(\d+\.\d+\.\d+\.\d+)\.(\d+)\s>\s(\d+\.\d+\.\d+\.\d+)\.(\d+):\sUDP,\slength\s(\d+)')
        sensor_packets = defaultdict(list)

        with open(file_path, 'r') as file:
            print(f'Lines parsed: self._tcpdump_lines_parsed')
            lines = file.readlines()
            if len(lines) > self._tcpdump_lines_parsed:
                lines = lines[self._tcpdump_lines_parsed]
                total_lines = len(lines)
                self._tcpdump_lines_parsed += total_lines
                for i, line in enumerate(lines):
                    if i % 10000 == 0:
                        print(f"Parsing progress: {i}/{total_lines} lines ({i/total_lines*100:.2f}%)")
                    match = udp_pattern.search(line)
                    if match:
                        time_str = match.group(1)
                        src_ip = match.group(2)
                        packet_size = int(match.group(6))
                        timestamp = datetime.strptime(time_str, '%H:%M:%S.%f')
                        sensor_packets[src_ip].append((timestamp, packet_size))

        print(f"Parsing completed in {time.time() - start_time:.2f} seconds")
        return sensor_packets

    def _aggregate_throughput(self, sensor_ip, packets, interval=1):
        print("Aggregating throughput...")
        start_time = time.time()
        if not packets:
            return []

        packets.sort(key=lambda x: x[0])
        start_time_packet = packets[0][0]
        end_time_packet = packets[-1][0]
        current_time = start_time_packet
        #throughput_data = []

        total_intervals = int((end_time_packet - start_time_packet).total_seconds() / interval)
        processed_intervals = 0

        while current_time <= end_time_packet:
            next_time = current_time + timedelta(seconds=interval)
            interval_packets = [p for p in packets if current_time <= p[0] < next_time]
            total_data = sum(p[1] for p in interval_packets) * 8  # Convert to bits
            throughput = total_data / interval  # bits per second
            self._throughput_data[sensor_ip].append((current_time, throughput / 1e6))  # Convert to Mbps
            current_time = next_time

            processed_intervals += 1
            if processed_intervals % 100 == 0:
                print(f"Aggregation progress: {processed_intervals}/{total_intervals} intervals ({processed_intervals/total_intervals*100:.2f}%)")

        print(f"Aggregation completed in {time.time() - start_time:.2f} seconds")
        #return throughput_data

    def _create_plot(self, data, output_name, timestamps, xlabel, ylabel, title):
        print(f"Plotting {output_name}...\n")

        plt.figure(figize=(12,6))
        plt.plot(data, self.time_stamps)
        plt.title(title)
        plt.xlabel(xlabel)
        plt.ylabel(yabel)
        plt.title(title)
        plt.grid(True)
        output_path = f"{self.output_dir}/output_name.png"
        plt.savefig(output_path)
        plt.close()
        print(f"Done plotting\n")

    
    def _plot_rates(self, sampling_freq):
        print("Plotting rates...")
        rates = zip(*self.rates)
        time = [t + 1 for t in range(len(rates))]
        num_sensors_with_rate = []

        for t in time:
            num_sensors_with_rate.append([rates[t].count(n) for n in range(sampling_freq)])
        num_sensors_with_rate = zip(*num_sensors_with_rate)

        plt.figure(figsize=(12, 6))
        plt.xlabel('Time (seconds)')
        plt.ylabel('Number of Sensors with Rate)')
        plt.title('Resdiual Energy over Time for All Sensors')
        
        for rate in range(sampling_freq):
            plt.plot(time, num_sensors_with_rate[rate], label=f'self._rate_sizes[rate] Kbps')

        plt.grid(True)
        plt.legend()
        output_path = f"{self.output_dir}/rates.png"
        plt.savefig(output_path)
        plt.close()
        print(f"Plot for all sensors saved to {output_path}")

    def _plot_energy(self):
        print("Plotting energy...")

        plt.figure(figsize=(12, 6))
        energy = [np.average(e) for e in zip(*self.energy_log)]
        time = [t + 1 for t in range(len(energy))]
        plt.plot(time, energy)
        plt.xlabel('Time (seconds)')
        plt.ylabel('Resdiual Energy (?)')
        plt.title('Resdiual Energy over Time for All Sensors')
        plt.grid(True)
        output_path = f"{self.output_dir}/energy_plot_all_sensors.png"
        plt.savefig(output_path)
        plt.close()
        print(f"Plot for all sensors saved to {output_path}")

        for sensor in range(sensor_id):
            plt.figure(figsize=(12,6))
            energy = [e[i] for e in zip(*self.energy_log)]
            plt.plot(time, energy, label=sensor_id)
            plt.xlabel('Time (seconds)')
            plt.ylabel('Resdiual Energy')
            plt.title(f'Resdiual Energy for Sensor {sensor_id}')
            plt.legend()
            plt.grid(True)
            output_path = f"{self.output_dir}/resdiual_energy_plot_{sensor_id}.png"
            plt.savefig(output_path)
            plt.close()
            print(f"Plot for sensor {sensor_id} saved to {output_path}")

    """
    def _plot_throughput(self):
        print("Plotting throughput...")
        output_dir = f'self.{log_directory}/graphics'

        plt.figure(figsize=(12, 6))
        throughput = [np.sum(throughputs) for throughputs in zip(*self.throughputs)]
        time = [t + 1 for t in range(len(throughput)]
        plt.plot(time, throughput)
        plt.xlabel('Time (seconds)')
        plt.ylabel('Throughput (Kbps)')
        plt.title('Throughput over Time for All Sensors')
        plt.grid(True)
        output_path = f"{output_dir}/throughput_plot_all_sensors.png"
        plt.savefig(output_path)
        plt.close()
        print(f"Plot for all sensors saved to {output_path}")

        for sensor in range(sensor_id):
            plt.figure(figsize=(12,6))
            throughput = [throughputs[i] for throughputs in zip(*self.throughputs)]
            plt.plot(time, throughput, label=sensor_id)
            plt.xlabel('Time')
            plt.ylabel('Throughput (Kbps)')
            plt.title(f'Throughput Over Time for Sensor {sensor_id}')
            plt.legend()
            plt.grid(True)
            output_path = f"{output_dir}/throughput_plot_{sensor_id}.png"
            plt.savefig(output_path)
            plt.close()
            print(f"Plot for sensor {sensor_id} saved to {output_path}")
        """

    """
    def main():
        input_file = '/mydata/mydata/RL_agent/output/extracted_data/tcpdump_output_capture.txt'
        output_dir = '/mydata/mydata/RL_agent/output/extracted_data'

        overall_start_time = time.time()

        sensor_packets = self._parse_tcpdump_output(input_file)
        sensor_throughput = {}

        print(f"Processing throughput for {len(senor_packets)} sensors...")
        for i, (sensor_ip, packets) in enumerate(sensor_packets.items()):
            print(f"Processing sensor {i+1}/{len(sensor_packets)}: {sensor_ip}")
            self._aggregate_throughput(sensor_ip, packets)
            #sensor_throughput[sensor_ip] = throughput_data

        #plot_throughput(sensor_throughput, output_dir)

        print(f"Total execution time: {time.time() - overall_start_time:.2f} seconds")
    """

#if __name__ == '__main__':
#    setLogLevel('info')
#    cluser_id = 0 
#    sensor_ids = range(5, 15)
#    cluster = sensor_cluster(1, sensor_ids, server_ip, port, log_director=f'data/c{cluster_id}/log')
#    cluster.start()

