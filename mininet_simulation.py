import sys
import socket
import os
import time
import threading
import random
import multiprocessing
import json
import subprocess
import struct
import numpy as np
import re
import matplotlib.pyplot as plt
import time
import math
import pickle
import networkx as nx
from networkx.algorithms.clique import find_cliques as maximal_cliques
from itertools import combinations
from ortools.linear_solver import pywraplp 

from tqdm import tqdm
from mininet.node import Controller
from mininet.log import setLogLevel, info
from mn_wifi.net import Mininet_wifi
from mn_wifi.cli import CLI
from mn_wifi.link import wmediumd
from mn_wifi.wmediumdConnector import interference
from datetime import datetime, timedelta
from collections import defaultdict


"""Convert throughput from KiB, MiB, B to KB."""
def convert_to_KB(throughput):
    match = re.match(r"([0-9.]+)([KMG]i?B?/s?)", throughput.strip())
    if match:
        value = float(match.group(1))
        unit = match.group(2).lower()
        if 'ki' in unit:  # KiB
            return value
        elif 'mi' in unit:  # MiB
            return value * 1024 
        elif 'b' in unit:  # B
            return value / 1024
        else:
            return 0
    return 0

"""Extract the latest throughput from file."""
def get_latest_throughput(filename):
    with open(filename, 'r') as f:
        lines = f.readlines()
    
    # Get the last non-empty line with throughput
    latest_line = lines[-1].strip()
    
    # Extract the throughput value
    throughput_value = latest_line.strip('()')  # Remove the square brackets
    
    # Convert throughput to bytes
    throughput_KB = convert_to_KB(throughput_value)
    
    return throughput_KB 

class sensor_cluster():
    def _read_temperature_data(self, file_path, sensor_id):
        with open(file_path, 'r') as file:
            # Skip the first 4 lines
            data = []
            bytes_received = 0
            for line in file:
                bytes_received += len(line)
                # Ignore filler lines
                if line[0] == 'G':
                    continue
                try:
                    # Split the line and try to convert the temperature value (9th column, index 8) to float
                    temperature = np.float32(line.strip().split(',')[8])
                    data.append(temperature)
                except (ValueError, IndexError):
                    # If conversion fails or the line doesn't have enough columns, skip this line
                    continue
            self._throughputs[sensor_id] = bytes_received / 1024 / self._observation_time

        return np.array(data)

    """
    Split dataset into chunks. A chunk corresponds to a transmissions worth of data.
    Note: The chunks returned must be padded before transmission to the correct size.

    Args:
        dataset_dir (string): Path to dataset file
        sensor_id (int): ID of the sensor associcated to the dataset file
        file_lines_per_transmission (int): How many file lines to include per transmission 

    Returns:
        String List: List of chunks 
    """

    def _preprocess_dataset_into_chunks(self, dataset_dir, sensor_id, file_lines_per_chunk):
        # Cache the dataset into memory 
        with open(dataset_dir, 'r') as file:
            lines = file.readlines()

        # Get the number of lines in the file 
        file_size = len(lines)

        # Get the number of chunks the data set can split into
        num_chunks_in_file = file_size // file_lines_per_chunk

        if num_chunks_in_file < self._num_transmission_frames:
            print(f'The number of chunks to send or the number of file lines per chunk must be decreased. File {dataset_dir} contains enough data for {num_chunks_in_file} but the number of transmission frames is {self._num_transmission_frames}')

        # Split file into chunks
        chunks = [lines[i:i + file_lines_per_chunk] for i in range(0, len(lines), file_lines_per_chunk)]
        chunks = [''.join(chunk) for chunk in chunks[:self._num_transmission_frames]]

        return chunks
   
    """
    Get an observation for an rl-scheduler.

    Returns:
        tuple: (rates_id, similarity, energy, throughputs, reward)
            rates_id - ID of rates used during the observation
            similarity - num_sensor x num_sensor matrix where entry i,j denotes wheter the data generated by sensor i was similar to the data generated by sensor j
            energy - percent of energy remaining for each sensor
            throughputs - vector of average throughput for each sensor
            reward - reward according to reward function
    """

    def get_obs(self):
        print('\n\nGetting observation')

        # Dict with keys as awake sensor ids and values as the data received by the cluster head from a sensor
        temperature_data = {}

        for i in range(self._num_sensors):
            # Name of file where transmisions recieived by the cluster head from sensor i is stored
            file_name = f'sensor_{self._sensor_ids[i]}.txt'

            # Create a copy of the original file
            subprocess.run(["cp", f'{self._log_directory}/ch_received_data/{file_name}', f'{self._log_directory}/ch_received_data/.{file_name}'])
            file_path = os.path.join(self._log_directory, f'ch_received_data/.{file_name}')

            # Clear the file for future transmissions
            # TODO: Lock before clearing?
            with open(f'{self._log_directory}/ch_received_data/{file_name}', 'r+') as file:
                file.truncate(0)
            
            # Read the temperature data for sensor i from the copied file
            sensor_data = self._read_temperature_data(file_path, i)
            if len(sensor_data) > 0:
                temperature_data[i] = sensor_data
                data_arrived = True

        similarity = np.zeros((self._num_sensors, self._num_sensors))
        reward = 0

        awake_sensors = list(temperature_data.keys())
         
        # Create a graph with veritices representing sensors and edges denoting similarity
        G = nx.Graph()

        print(f'Awake sensors: {awake_sensors}')
        
        for i in range(self._num_sensors):
            G.add_node(i)

        for i in range(len(awake_sensors)):
            print(f"throughput in bytes for sensor {awake_sensors[i]}: {self._throughputs[i]}")
            # Get the average throughput in KiB/s 
            #self._throughputs[awake_sensors[i]] = len(temperature_data[awake_sensors[i]]) / 1024 / self._observation_time

            for j in range(i + 1, len(awake_sensors)):
                # TODO: Currently only using first data point for similiarity; need to add timestamps to data for more robust similarity checking
                similarity[awake_sensors[i], awake_sensors[j]] = int(temperature_data[awake_sensors[i]][0] - temperature_data[awake_sensors[j]][0] <= self._similarity_threshold)
                if similarity[awake_sensors[i],awake_sensors[j]] == 1:
                    print(f'Node {awake_sensors[i]} ~ Node {awake_sensors[j]}')
                    G.add_edge(awake_sensors[i],awake_sensors[j])
            
        print(f'Rates (Kib/s) : {[a for a in self.transmission_freq_idxs]}')
        max_throughput = (max(self._throughputs))

        self._max_throughput = max(max(self._throughputs), self._max_throughput)
        self._max_total_throughput = max(self._max_total_throughput, sum(self._throughputs))

        if self._max_throughput == 0:
            return (similarity, [e / self._full_energy for e in self._energy], self._throughputs, 0)

        
        print(f'Throughputs (Kib/s) : {self._throughputs}')
        print(f'Total throughput over observation: {np.sum(self._throughputs)} KiB/s')

        # Get a maximal clique cover for G
        maximal_clique_cover = list(maximal_cliques(G)) 
        print(f'maximal_clique_cover: {maximal_clique_cover}')
        print(f'Max throughput in clique: {[max([self._throughputs[node] for node in clique]) for clique in maximal_clique_cover]}')

        max_clique_throughputs = [max([self._throughputs[node] / (self._max_throughput) for node in clique]) for clique in maximal_clique_cover] 
        clique_throughput_reward = 0.5 * (np.median(max_clique_throughputs) - 1)
        throughput_reward = 0.5 * ((np.sum(self._throughputs) / (self._max_total_throughput)) - 1)
        reward = clique_throughput_reward + throughput_reward

        if np.sum(self._throughputs) > self._max_throughput:
            self._max_throughput = np.sum(self._throughputs) 

        print(f'Weighted avg energy: {np.average(self._energy)/100}')
        print(f'Reward: {reward}')
        print(f'Clique reward: {clique_throughput_reward}')
        print(f'Throughput reward: {throughput_reward}')

        #for i in range(self._num_sensors):
            #self.throughput_log[i].append(self._throughputs[i])
            #self.energy_log[i].append(self._energy[i])
            #self.rate_log[i].append(self.transmission_freq_idxs[i])

        #self.clique_log.append(maximal_clique_cover)

        #self.reward_log.append(reward)
        #self.clique_reward_log.append(clique_throughput_reward)
        #self.throughput_reward_log.append(throughput_reward)
        
        return (similarity, [e / self._full_energy for e in self._energy], self._throughputs, reward)
   
    """
    Create Mininet topology
    """
    def _create_topology(self):
        # build network
        self._net = Mininet_wifi(controller=Controller, link=wmediumd, wmediumd_mode=interference)

        info("*** Creating nodes\n")
        # create accesspoint
        self._net.addAccessPoint(f'ap23', ssid=f'new-ssid', mode='g', channel='5', position='50,50,0', cpu=1, mem=1024*2)
       
        #create cluster head
        self._cluster_head = self._net.addStation(f'ch', ip=f'192.168.0.100/24',
                                      range='150', position='30,30,0', cpu=1, mem=1024*2)

        #create 
        self._sensors = []
        for i in range(self._num_sensors):
            ip_address = f'192.168.0.{i + 1}/24'
            self._sensors.append(self._net.addStation(f's{i}', ip=ip_address, range='116', position=f'{-30 - i},-30,0', cpu=1, mem=1024*2))

        info("*** Adding Controller\n")
        self._net.addController(f'c0')

        info("*** Configuring wifi nodes\n")
        self._net.configureWifiNodes()

        self._net.build()
        self._net.start()


    def __init__(self, sensor_ids, transmission_size=2*1024, transmission_frame_duration=1, num_transmission_frames=10000, observation_time=10, file_lines_per_chunk=10, log_directory='data/log', dataset_directory='data/towerdataset'):
        # Simulation parameters 
        self._transmission_size = transmission_size # in bytes
        self._sensor_ids = sensor_ids # used to find data file names
        self._similarity_threshold = 1
        self._num_transmission_frames = num_transmission_frames 
        self._observation_time = observation_time # The number of seconds between each observation

        self._num_sensors = len(sensor_ids)
        
        self._throughputs = [0 for i in range(self._num_sensors)]
        self._max_throughput = 0
        self._max_total_throughput = 0
        self._transmission_frame_duration = transmission_frame_duration

        # Directories
        self._log_directory = log_directory
        self._dataset_directory = dataset_directory
        self._output_dir = f'{self._log_directory}/output/extracted_data' # Where to output logs 
       
        # Rate configuration
        self.transmission_freq_idxs = multiprocessing.Array('i', [2] * self._num_sensors)
        self._transmission_frequencies = np.array([0, 1, 2, 4]) # possible transmit frequencies transmissions per second)

        # Energy configuration
        self._full_energy = 100
        self._recharge_time = 10 * transmission_frame_duration
        self._recharge_threshold = 20
        self._energy = [self._full_energy for _ in range(self._num_sensors)]

        # Data logs 
        self.rate_log = [[] for _ in range(self._num_sensors)]
        self.energy_log = [[] for _ in range(self._num_sensors)]
        self.throughput_log = [[] for _ in range(self._num_sensors)]
        self.reward_log = []
        self.clique_reward_log = []
        self.throughput_reward_log = []
        self.clique_log = []

        # Delete the log folder if already it exists
        subprocess.run(["rm", "-rf", log_directory])
        os.makedirs(log_directory)
        os.makedirs(log_directory + '/ch_received_data')
        os.makedirs(log_directory + '/error')
        os.makedirs(log_directory + '/pcaps')

        if not os.path.exists(dataset_directory):
            print("Dataset directory does not exist. Exiting now")
            exit()

        # Initalize the dataset for each sensor 
        self._datasets = {}
        for i in range(self._num_sensors):
            tower_number = i + 2  # Start from tower2 
            file_path = f'{dataset_directory}/tower{tower_number}Data_processed.csv'
            if os.path.exists(file_path):
                self._datasets[i] = self._preprocess_dataset_into_chunks(file_path, i, file_lines_per_chunk)
                self._datasets[i] = self._datasets[i] + self._datasets[i]
            else:
                print(f"Warning: Dataset file not found for sensor {i}: {file_path}")
                self._datasets[i] = []

        self._create_topology()


    """
    Send message from a sensor to the cluster head

    Args:
        sensor (station): sensor to send message from
        ch_ip (string): cluster head ip
        sensor_idx (int): index of sensor
    """

    def _send_messages_to_cluster_head(self, sensor, ch_ip, sensor_idx):
        # Get the chunks corresponding to this sensor
        chunks = self._datasets[sensor_idx] 
        info(f"Number of Chunks: {len(chunks)}\n")

        if not chunks:
            info(f"Sensor {sensor_idx}: No data available. Skipping send_messages.\n")
            return
        
        # Create a file to store the packets received by the cluster head
        sensor.cmd(f'touch {self._log_directory}/ch_received_data/sensor_{self._sensor_ids[sensor_idx]}.txt')
       
        # Track the number of chunks sent 
        chunks_sent = 0

        # Index of the next chunk to send 
        next_chunk_idx = 0

        # Port to communicate with the cluser head on 
        port = 5001 + sensor_idx 
        
        # Initalize the sensors energy and the recharge count 
        energy = self._full_energy
        self._energy[sensor_idx] = self._full_energy
        charge_count = 0

        # Log the initial transmission rate
        self.rate_log[sensor_idx].append(self.transmission_freq_idxs[sensor_idx])

        filler = 'G' * (self._transmission_size - 1) + '\n'

        while next_chunk_idx < len(chunks):
            # Recharge sensor if energy is below the recharge threshold
            if self._energy[sensor_idx] < self._recharge_threshold:
                charge_count += 1

                recharge_time = time.time()
                rechar_time_stamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(recharge_time))
                info(f'Sensor {sensor_idx}: Energy {self._energy[sensor_idx]} below threshold ({self._recharge_threshold}). Recharging...')
                time.sleep(self._recharge_time)

                # Skip chunks that could have been transmitted during the recharge time 
                next_chunk_idx += (self._recharge_time / self._transmission_frame_duration) // max(self._transmission_frequencies)
                                
                # Update the sensors energy and log the new energy level
                self._energy[sensor_idx] = self._full_energy
                sensor.energy = energy

                info(f'Sensor {sensor_idx}: Energy Recharged to full energy ({self._full_energy}), Current time: {recharge_time}, Charge Count: {charge_count}. Resuming operations.')
                
                if next_chunk_idx >= len(chunks):
                    self._transmit_data_status[sensor_idx].set()
                    break

            # Get the sensors current transmission rate
            transmit_rate = self._transmission_frequencies[self.transmission_freq_idxs[sensor_idx]] 

            # Store the current time
            transmission_start_time = time.time()
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(transmission_start_time))
            ms = int((transmission_start_time - time.time()) * 1000)

            # Sensor should skip tranmission during the current frame
            if transmit_rate == 0:
                next_chunk_idx += int(max(self._transmission_frequencies))

                time.sleep(self._transmission_frame_duration)
                continue

            # Get the chunk to send
            info(f"next chunk idx: {next_chunk_idx}")
            chunk = chunks[int(next_chunk_idx)] 
         
            # Send the chunk (with filler to pad the chunk to the correct length)
            cmd = f'printf "{chunk}\n{filler[len(chunk):]}\n" | nc -v -q0 -u {ch_ip} {port} >> {self._log_directory}/error/nc{sensor_idx} 2>&1 &'
            sensor.cmd(cmd)

            

            chunks_sent += 1

            transmission_time = time.time() - transmission_start_time
            info(f'Transmission time: {transmission_time}\n')

            sleep_time = self._transmission_frame_duration / transmit_rate
            
            """
            Chunks correspond with data for units of time i.e. chunk[0] corresponds to time 0, chunk[1] corresponds to time 1, 
            and so on. max(self._transmission_rates) is the maximum number of transmissions that could occur during a transmission 
            frame. transmit_rate is the number of tranmissions per frame currently made by the sensor. 
            (max(self._transmission_rates)  / transmit_rate) - 1 is therefore the number of tranmissions that are skipped per frame.   
            """

            next_chunk_idx += (max(self._transmission_frequencies) / transmit_rate) - 1
            time.sleep(sleep_time)

        info(f"Sensor {sensor_idx}: Finished sending messages\n")


    """
    Start background netcat listining process for cluster head

    Args:
        node (station): cluster head 
    """
    def _receive_messages(self, node):
        base_output_file = f'{self._log_directory}/ch_received_data/sensor'

        for i in range(self._num_sensors):
            # Create a file to store data received from sensor i
            output_file = f'{base_output_file}_{self._sensor_ids[i]}.txt'
            node.cmd(f'touch {output_file}')

            # Create a listener for sensor i 
            node.cmd(f'nc -n -vv -ul -p {5001 + i} -k >> {output_file} 2> {self._log_directory}/error/listen_err &')
            info(f"Receiver: Started listening on port {5001 + i} for sensor {i}\n")

        # Capture the network by pcap
        pcap_file = f'{self._log_directory}/pcaps/capture.pcap'
        node.cmd(f'tcpdump -i {node.defaultIntf().name} -n udp portrange {5001}-{5001+self._num_sensors-1} -U -w {pcap_file} &')
        info(f"Receiver: Started tcpdump capture on ports 5001-{5001+self._num_sensors - 1}\n")

    """
    Kill netcat listining process for cluster head

    Args:
        node (station): cluster head 
    """
    def _stop_receivers(self, node):
        # Kill the receiver
        node.cmd('pkill -f "nc -ul"')
        # Kill the tcpdump capture
        node.cmd('pkill tcpdump')
        info("Stopped all nc receivers and tcpdump\n")

    """
    Begin message transmission from sensors to cluster head and reap all threads after transmission concludes.   
    """
    def start(self):
        print("STARTING")
        
        info("*** Setting up communication flow\n")
        try:
            info("*** Starting receivers\n")
            print("Starting receivers")
            
            # Activate cluster head listening threads
            receive_thread = threading.Thread(target=self._receive_messages, args=(self._cluster_head,))
            receive_thread.start()

            self._start_time = time.time()

            # Give listening threads time to start
            time.sleep(10) 
            
            print("Starting senders")
            info("*** Starting senders\n")
            sender_threads = []
            listener_threads = []
            ch_ip = f'192.168.0.100'

            for i, sensor in enumerate(self._sensors):
                tcpdump_file = f'{self._log_directory}/pcaps/tcpdump_sender_sensor{i}.pcap'
                sensor.cmd(f'tcpdump -i s{i}-wlan0 -w {tcpdump_file} &')
                
                thread = threading.Thread(target=self._send_messages_to_cluster_head, args=(sensor, ch_ip, i))
                thread.start()
                sender_threads.append(thread)

            time.sleep(self._observation_time)
           
            # Give cluster head time to receive data
            time.sleep(2 * self._observation_time)

            for thread in sender_threads:
                thread.join()

            print(f'Simulation complete; saving data\n')
            with open('figure_data.pkl', 'wb') as file:
                pickle.dump((self._sensor_ids, self._rate_frequencies, self.rate_log, self.energy_log, self.throughput_log, self.reward_log, self.clique_reward_log, self.throughput_reward_log, self.clique_log), file)
            print(f'Pickle dump succesfuly made\n')

            self._stop_receivers(cluster_head)
            receive_thread.join()

            for thread in listener_threads:
                thread.join()
        
            for sensor in sensors:
                sensor.cmd('pkill tcpdump')
            cluster_head.cmd('pkill nc')

            self._plot_energy()
            self._plot_throughput()
            self._plot_rates(3)
            self._create_plot(self, self.rewards, 'rewards', self.timestaps, 'Time (seconds)', 'Reward', 'Reward over Time')
            self._create_plot(self, self.similarity, 'Similarity Penalty', self.timestaps, 'Time (seconds)', 'Penalty', 'Similarity Penalty over Time') 
            
        except Exception as e:
            info(f"*** Error occurred during communication: {str(e)}\n")
            
        info("*** Running CLI\n")
        CLI(self._net)

        info("*** Stopping network\n")
        self._net.stop()

        self._server.close()

    def _parse_tcpdump_output(self, file_path):
        print("Starting to parse tcpdump output...")
        start_time = time.time()
        udp_pattern = re.compile(r'(\d{2}:\d{2}:\d{2}\.\d+)\sIP\s(\d+\.\d+\.\d+\.\d+)\.(\d+)\s>\s(\d+\.\d+\.\d+\.\d+)\.(\d+):\sUDP,\slength\s(\d+)')
        sensor_packets = defaultdict(list)

        with open(file_path, 'r') as file:
            print(f'Lines parsed: self._tcpdump_lines_parsed')
            lines = file.readlines()
            if len(lines) > self._tcpdump_lines_parsed:
                lines = lines[self._tcpdump_lines_parsed]
                total_lines = len(lines)
                self._tcpdump_lines_parsed += total_lines
                for i, line in enumerate(lines):
                    if i % 10000 == 0:
                        print(f"Parsing progress: {i}/{total_lines} lines ({i/total_lines*100:.2f}%)")
                    match = udp_pattern.search(line)
                    if match:
                        time_str = match.group(1)
                        src_ip = match.group(2)
                        packet_size = int(match.group(6))
                        timestamp = datetime.strptime(time_str, '%H:%M:%S.%f')
                        sensor_packets[src_ip].append((timestamp, packet_size))

        print(f"Parsing completed in {time.time() - start_time:.2f} seconds")
        return sensor_packets

    def _aggregate_throughput(self, sensor_ip, packets, interval=1):
        print("Aggregating throughput...")
        start_time = time.time()
        if not packets:
            return []

        packets.sort(key=lambda x: x[0])
        start_time_packet = packets[0][0]
        end_time_packet = packets[-1][0]
        current_time = start_time_packet
        #throughput_data = []

        total_intervals = int((end_time_packet - start_time_packet).total_seconds() / interval)
        processed_intervals = 0

        while current_time <= end_time_packet:
            next_time = current_time + timedelta(seconds=interval)
            interval_packets = [p for p in packets if current_time <= p[0] < next_time]
            total_data = sum(p[1] for p in interval_packets) * 8  # Convert to bits
            throughput = total_data / interval  # bits per second
            self._throughput_data[sensor_ip].append((current_time, throughput / 1e6))  # Convert to Mbps
            current_time = next_time

            processed_intervals += 1
            if processed_intervals % 100 == 0:
                print(f"Aggregation progress: {processed_intervals}/{total_intervals} intervals ({processed_intervals/total_intervals*100:.2f}%)")

        print(f"Aggregation completed in {time.time() - start_time:.2f} seconds")
